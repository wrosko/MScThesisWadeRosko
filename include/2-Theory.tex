
\chapter{Theory}


This thesis utilizes techniques for interpreting correlations from compositional data from two different cohorts. The statistics based methods of \acrshort{SparCC} and \acrshort{FastSpar} (Section \ref{lit-sparcc}) transform the compositional data into the two respective correlation networks. Then, overlapping sub-graphs are generated between the two networks which allows for the networks to be compared and for the identification of ''driver`` nodes to be identified. This comes from the general methodology from NetShift (\ref{lit-netshift}).

% This one not needed for now, but think about robust correlation maybe?
% \section{The Graphical Lasso}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{SparCC and FastSpar} \label{theory-sparcc}

\acrshort{SparCC} utilizes the log-ratio transformation to create correlation networks from compositional data. This is a standard technique used in compositional data in order to reduce compositional effects and the possible impact that rare taxa may have in a correlation analysis. Using the log-transformed components of the data, \acrshort{SparCC} estimates the Pearson correlations to an approximation based upon the assumptions that the number of components is large, and the correlation network is actually sparse \citep{Friedman2012}. \acrshort{FastSpar} follows the same methods as the \acrshort{SparCC} algorithm, but differs in the implementation of assessing the statistical significance. Both of these 

% \subsection{}
\subsection{Bayesian Approach to Estimate Component Fraction}\label{theory-bayes}
Initial normalization of the compositional data is required for later stages in the \acrshort{SparCC} algorithm, however the maximum-likelihood estimate of normalizing by the total counts in the sample is unreliable. This method overestimates the total number of zero fractions from rare taxa or variations in sequencing depth \citep{Agresti2005,Friedman2012}. Instead, the authors estimate the component fractions with a Bayesian approach by utilizing the Dirichlet distribution.

The \acrshort{SparCC} approach utilizes the full joint distribution of fractions generated by the Dirichlet distribution. Generally the Dirichlet distribution is defined as $\text{Dir}\left(\alpha\right)$ where $\alpha$ is a parametric vector of positive reals. Using the Gamma distribution:
\begin{equation}\label{eq:gamma}
    \Gamma\left(\alpha\right) = \int_0^{\infty} t^{\alpha-1} e^{-t} dt,
\end{equation}
the Dirichlet probability density function can be formally defined as:
\begin{equation}\label{eq:-dirich}
    \text{Dir}\left( \alpha \right) = f\left(\alpha^k, y^k\right) = \frac{\Gamma \left( \alpha_0 \right)}{\prod^k_{i=1}\Gamma \left( \alpha_i \right)} \prod_{i=1}^k y_i^{\alpha_i -1}
\end{equation}
where $\alpha^k$ is the parameter vector of positive reals containing $k$ components, $y_i$ is the associated vector element, and $\alpha_0$ is the summation $\alpha_0 = \sum_{i=1}^k \alpha_i$ \citep{Lin2016}. \acrshort{SparCC} thus samples from this distribution function to estimate the true fractions from the obseved compositional counts. 

If the set of data is considered as the vectors for the components $i$ (in this case each \acrshort{OTU} is a component), then the fractions distribution is the Dirichlet distribution \citep{Gelman2013}:
\begin{equation}\label{eq:dirichlet}
    p\left(\underline{x}|\underline{N}\right)=\text{Dir}\left(\underline{N}+1\right),
\end{equation}
where $\underline{x}$ is the components' true fractions and $\underline{N}$ is the observed counts. Note the $\underline{N}+1$ component in the Dirichlet distribution; this added value is defined as adding a \ital{pseudocount} of 1 to all count values for each component\footnote{Pseudocounts are used in compositional data to avoid division by zero errors that could occur with generally sparse data. They are usually small values compared to the rest of the data in order to minimize the effects of influencing fraction calculations. Different methods have suggested various optimal pseudocount values, but the authors use a pseudocount of 1 for this method. For more examples of pseudocounts refer to their usage in these papers: \citet{Weiss2017, Mandal2015, Wang2017}.}.


\subsection{Log Ratio}\label{theory-log}
Compositional data is frequently normalized by employing the use of the \acrlong{CLR}. Consider an $m \times n$ matrix where the $m$ rows are samples, and the $n$ columns are features of the data. Since this theory is applied to 16S \acrshort{rRNA} compositional data, the theory will be presented in relation to 16S data, but any type of compositional data should be applicable. Thus, imagine an \acrshort{OTU} table with $m$ samples, and $n$ \acrshort{OTU} IDs.

% \ital{\textbf{*Maybe Define Subcompositional coherence? *}}

The log-ratio transformation is defined as:
\begin{equation} \label{eq:log}
    y_{ij} = \log \frac{x_i}{x_j} = \log x_i -\log x_j,
\end{equation}
where $y_{ij}$ contains the new information of the true abundances of \acrshort{OTU}s since the fraction of \acrshort{OTU} $x_i$ and \acrshort{OTU} $x_j$ is equal to the ratio of the true abundances. Additionally, the log-ratio transformation allows the resulting value to be independent of the other \acrshort{OTU}s in the analysis, and $y_{ij}$ is no longer limited to the simplex of the dimensionality of the components $i$ and $j$ in the range $\{1,2,...,n\}$. 

\subsection{Compositional Based Dependencies}\label{theory-dependencies}


From here, the value that should be used in the analysis comes from the variance of the log-ratio applied across all samples \citep{Aitchison2003}:

\begin{equation}\label{eq:tij_variance}
    t_{ij} \equiv \text{Var}\left[ \log \frac{x_i}{x_j} \right] = \text{Var} \left[ y_{ij} \right].
\end{equation}
\citeauthor{Aitchison2003} uses this value $t_{ij}$ to describe the dependencies between the components and calls this resulting matrix the \ital{variation matrix} ($\textbf{T}$). Note that this relation will result in $t_{ij}=0$ for perfectly correlated \acrshort{OTU}s and will vary with other correlation relations \citep{Friedman2012}. A basic property of variance and covariance for the sum of two random variables $X$ and $Y$ leads to the relation:
\begin{equation}\label{eq:var_relation}
    \text{Var}\left[aX-bY \right] = a^2 \text{Var}\left[ X \right] + b^2 \text{Var} \left[Y\right]-2ab \text{Cov} \left[X, Y\right],
\end{equation}
where $a$ and $b$ are both constants. This property will be applied in Equation \ref{eq:basis_correlation}. 

\subsection{Basis Correlation Relation}\label{theory-basiscorr}

In order to better understand the scaling of the dependency value and determine what constitutes a weak or strong dependence, we need to relate the value to the correlation of the true abundances in the compositional data. To relate Equation \ref{eq:tij_variance} to the basis correlation, first apply the relation of Equation \ref{eq:var_relation}:
\begin{equation}\label{eq:basis_correlation}
    \begin{split}
        t_{ij} & \equiv \text{Var}\left[ \log \frac{x_i}{x_j}\right] = \text{Var}\left[ \log \frac{w_i}{w_j}\right]=\text{Var}\left[ \log w_i - \log w_j \right], \\
        & = \text{Var}\left[ \log w_i\right]+\text{Var}\left[\log w_j\right] - 2\text{Cov}\left[\log w_i, \log w_j\right],
    \end{split}
\end{equation}
where $x_i = w_i$, $x_j = w_j$, and  $a,b = 1$. Then we can obtain the equation:
\begin{equation}\label{eq:abun_relation}
    t_{ij} = \omega^2_i + \omega^2_j - 2 \rho_{ij} \omega_i \omega_j,
\end{equation}
where $\omega_i^2$ and $\omega_j^2$ are the variances of the log-transformed variances of components $i$ and $j$ ($\text{Var}\left[ \log w_i\right]$ and $\text{Var}\left[\log w_j\right]$ respectively), and $\rho_{ij}$ is the correlation between them. 


\citeauthor{Friedman2012}'s aim is to infer the unobserved covariance matrix from the variation matrix $T$, but this is not possible in the general case because the basis variances are unknown. So an exact solution is not possible, but an approximation can be found\footnote{The approximation is valid for situations when there are many components that are sparsely correlated. This is assumed for sparse compositional data as seen in Eq. \ref{eq:approx_small}. If we were to assume that all of the basis variables have the same variance $\omega$, then we would have the relation $2 \gg 2 \langle \rho_{ij} \rangle_i$ and the \textbf{average} correlations would be small versus needing a specific \textbf{set of correlations} to be small \citep{Friedman2012}. }.
We initially rewrite Eq. \ref{eq:abun_relation} for $\rho_{ij}$:
\begin{equation}\label{eq:rho}
    \rho_{ij} = \frac{\omega_i^2 + \omega_j^2 -t_{ij}}{2\omega_i \omega_j},
\end{equation}
which gives the basis correlations if we have the variances $\omega_i$ and $\omega_j$ of the log-transformed basis and the quantity $t_{ij}$. To solve for these variables start with finding the approximation for the basis variance by defining the variation of component $i$ as:
\begin{equation}\label{eq:ti}
    \begin{split}
        t_i & \equiv \sum_{j=1}^{D}t_{ij}=d \omega^2_i + \sum_{j\neq i}\omega_j^2 -2\sum_{j\neq i} \rho_{ij}\omega_i \omega_j,
    \end{split}
\end{equation}
across all variances $D$ where $d\equiv D-1$. Then we factor Eq. \ref{eq:ti} by the term $d\omega_i^2$:
\begin{equation}
    t_i = d\omega_i^2  \left[ 1 + \frac{1}{d} \sum_{j \neq i}\frac{\omega_j^2}{\omega_i^2} -2 \frac{1}{d} \sum_{j \neq i}\rho_{ij} \frac{\omega_j}{\omega_i} \right],
\end{equation}
and then use $\langle \cdot \rangle_i$ notation to stand for the averaging of all pairs corresponding to $i$. Thus $t_i$ is now:
\begin{equation}\label{eq:ti_avg}
    t_i \equiv d\omega_i^2 \left[ 1 + \langle \left(\frac{\omega_j}{\omega_i}\right)^2 \rangle - 2 \langle \rho_{ij} \frac{\omega_j}{\omega_i} \rangle_i \right],
\end{equation}
and because we make the assumption that the correlation terms $\rho_{ij}$ are small:
\begin{equation}\label{eq:approx_small}
    1+\langle \left( \frac{\omega_j}{\omega_i} \right)^2 \rangle_i \gg 2\langle \rho_{ij}\frac{\omega_j}{\omega_i}\rangle_i,
\end{equation}
we are left with an expression for $t_i$ containing only the variances of the log-transformed basis:
\begin{equation}\label{eq:final_ti}
    t_i \simeq d\omega_i^2 + \sum_{j \neq i}\omega_j^2 \, , \; i = 1,2,...D.
\end{equation}

\subsection{Statistical Significance}\label{theory-signif}
In \acrshort{FastSpar}, statistical significance is calculated by sampling permutations of the data with replacement \citep{Watts2018}\footnote{This is described briefly in the paper, but can be found via the author's GitHub repository home directory \ital{README.md} file and source code in the  \ital{pvalue.cpp} script. Refer to the GitHub repository here: \url{https://github.com/scwatts/fastspar}.}. The \ital{p}-value implemented is termed the \ital{exact p-value} $p_e$ \citep{Phipson2010}. It's derived from a random sample of $m$ permutations with replacement, and the test statistics $t_{perm}$ may contain both repeat values and original observed values $t_{obs}$. Note that $p_e$ will be slightly less than $\frac{b+1}{m+1}$ for all $b = 1,2,...,m$ because the original data may be present at least once in one of the permutations.

If we let the exact permutation value $p_e$ be:
\begin{equation}\label{eq:p_e1}
    p_e = P\left( B \leq b \right),
\end{equation}
where $B$ is the number of permutations out of $m$ that have test statistics at least as extreme as $t_{obs}$, then we can find $p_e$ for any given $b$. Now let $B_t$ be the number of unique statistic values greater than $t_{obs}$, and consequently $p_t = \frac{B_t+1}{m_t+1}$. Assuming a true null hypothesis $H_0$, and $B_t = b_t$, then $B_t$ will follow a discrete uniform distribution over the integers $0,...,m_t$ and $B$ follows a binomial distribution with size $m$ and probability $p_t$. This is formally defined as:
\begin{equation}\label{eq:pe_2}
    p_e = \sum_{b_t=0}^{m_t} P\left(B \leq b|B_t = b_t \right) P\left(B_t = b_t| H_0 \right) = \frac{1}{m_t +1}\sum_{b_t=0}^{m_t} F\left( b;m,p_t \right),
\end{equation}
where $F()$ is the cumulative probability function of the binomial distribution \citep{Phipson2010}.

\subsection{SparCC and FastSpar}\label{theory-sparcc-algo}
By following the previous steps for the $t_i$ approximation, the basis correlations $\rho_{ij}$ can be calculated by using Eq. \ref{eq:rho}. The authors of \acrshort{SparCC} initially suggest a basic procedure to infer the correlations which follows as such:

\begin{enumerate}
\item  Estimate the component fractions with the Dirichlet distribution to obtain the fractions matrix $X$
\item Compute the variation matrix $T$ and the variations $t_i$
\item Plug the log-basis variances $\omega_i$ from Eq. \ref{eq:final_ti} into Eqs. \ref{eq:basis_correlation} and \ref{eq:abun_relation} to obtain $\rho_{ij}$
\item Iterate through steps 1-3, while each time identifying the most strongly correlated pairs of components and discarding those that form exclusive pairs
\item Estimate the fractions of remaining components and compute the new $t_i$, $\omega_i$, and $\rho_{ij}$
\end{enumerate}

The \acrshort{FastSpar} implementation, which this thesis utilizes, follows the above procedure but differs when assessing statistical significance as outlined in Section \ref{theory-signif}. Thus, following the above-mentioned procedures we obtain correlation values and their respective $p$-values. With this information in hand, we may analyze the newly generated correlation networks.

% \ref{eq:basis_correlation}}



\section{NetShift}\label{theory-Netshift}
The NetShift methodology presents a graph theoretic approach to quantifying differences in correlation networks. With its standard approach, it is possible to compare two similar types of networks.   

\subsection{General Properties}\label{theory-gen-props}
Given a graph $\bm{G}(\V,E)$, consider the general properties: degree, density, average path length, and cluster coefficient. The degree of a node $i$ in a graph is equivalent to the number of edges connected to the node and denoted by $k_i$. In a graph with nodes that are not self-connected, the maximum number of possible edges $E$ is limited to the number of nodes in the network as depicted in the relation $|\V|\left(|\V|-1\right)$. The graph density is defined as:
\begin{equation}\label{eq:density}
    D = \frac{2 |E|}{|\V|\left(|\V|-1\right)},
\end{equation}
where $\V$ and $E$ are the number of nodes and edges respectively. $D$ is dependent upon how connected the network is, and if the network is fully connected, $D = 1$. Now if we consider a node in the network $v_1$ and another node $v_2$, we can find the shortest path that connects the two. If the two are directly connected then $d\left(v_1, v_2\right) =1$, if there are multiple nodes between the two $d\left(v_1, v_2\right)$ will be the number of connecting edges, and if there is no path between the two $d\left(v_1, v_2\right)=0$. Thus we can then define the average path length $l_G$ of the network as:
\begin{equation}\label{eq:avg-path}
    l_G = \frac{1}{n\left(n-1 \right)}\sum_{i\neq j}d\left(v_1, v_2\right).
\end{equation}

Next, we will define the following properties: clustering coefficient, betweenness centrality, and coreness centrality. The local clustering coefficient is defined for each individual node in the network. It is dependent upon the number of edges connecting a node to its neighbors compared to the possible number of edges that it could have. For each node $i$, there exists a neighborhood $N_i$ which are the node's neighbors:
\begin{equation}\label{eq:n_i}
    N_i =    \{v_j : e_{ij} \in E \vee e_{ji} \in E \}.
\end{equation}
Using the neighborhood, the local clustering coefficient is defined as:
\begin{equation}\label{eq:C_i}
    C_i = \frac{2L_i}{k_i\left(k_i-1\right)},
\end{equation}
where $L_i$ is the number of edges in the neighborhood $N_i$ of node $i$ \citep{Watts1998,Barabasi2016}. Additionally, the average of the network clustering coefficients $\overline{C}$ can be calculated by averaging over all of the clustering coefficients:
\begin{equation}\label{eq:avg_ci}
    \overline{C} = \frac{1}{n}\sum_{i=1}^n C_i.
\end{equation}

Centrality measures in graphs aim to assign a value of importance in the network. One such relevant metric for real systems is the betweenness centrality, which is related to the number of shortest paths $l_G$ that pass through a given node. The higher the betweenness centrality, the more the node interacts with the rest of the network. This is an important metric for real-life systems as these nodes may drive the behavior of the network. Betweenness centrality is defined as:
\begin{equation}\label{eq:btween}
    g\left(\V\right) = \sum_{i\neq\V \neq j}\frac{\sigma_{ij}\left(\V \right)}{\sigma_{ij}},
\end{equation}
where $\sigma_{ij}$ is the number of shortest paths from $i$ to $j$, and $\sigma_{ij}\left( \V_i\right)$ is the number that pass through $\V$ \citep{Freeman1977}. Similarly to Equation \ref{eq:avg_ci}, we let the average betweenness centrality $\overline{g ( \V )}$ be:
\begin{equation}\label{eq:avg_btween}
    \overline{g ( \V )} = \frac{1}{n}\sum_{k=1}^n g\left(\V\right)_k.
\end{equation}

Another relevant metric is coreness centrality which observes the $k$-shell indices of a node's neighborhood \citep{Bae2014}. By taking a graph and its nodes, the $k$-shell indices are assigned by first removing all nodes from the graph whose degree is equal to $1$. These are then assigned to the $1$-core with the index 1. This method is done recursively, increasing $k$ each time until all nodes have been assigned to a $k$-core with a respective $k$-shell index between 1 and $k$ \citep{Bae2014,Lue2016,Dorogovtsev2006}. After assigning the $k$-shell indices, each node $\V$ can be assigned a neighborhood coreness value $C_{nc}$:
\begin{equation}\label{eq:n-coreness}
    C_{nc}\left( \V \right) = \sum_{w \in N\left(\V\right) } \text{k-s}\left(w\right),
\end{equation}
where $w$ is in the neighborhood of $\V$, and $\text{k-s}(w)$ is the $k$-shell index of node $w$. Either $C_{nc}$ or the extension of $C_{nc}$ to include the neighbors of the neighbors of $\V$ can be used as a metric.

\subsection{Quantifying Community Structure Change}\label{theory:struct-change}
While we could compare the two networks and their respective properties denoted in Section \ref{theory-gen-props}, these cannot be used solely to compare two networks if they contain different nodes. So, as stated previously, the network comparison needs to address the common sub-graphs of the networks.

By comparing the two sets, a baseline score called the \acrfull{JEI} can be found to quantify the re-wiring of edges, and highlight major shifts in the role of nodes in the networks. \acrshort{JEI} is defined as:
\begin{equation}\label{eq:JEI}
    \text{\acrshort{JEI}} = \frac{A_E \cap B_E}{A_E \cup B_E},
\end{equation}
where $A_E$ and $B_E$ are the edge sets for a control network $A$ and case network $B$ respectively. The authors suggest that the Jaccard index can be useful, but it does not describe the change between network $A$ and $B$ sufficiently so that the directionality of the changes is taken into account \citep{Kuntal2018}. For example, there may be a shift in the edges between a node in $A$, and the same node in $B$. Consider the case where we are observing the same node $n$ in the two networks ($A^n$ and $B^n$). Then the edge list of $A^n_E$ may not be equivalent to the edge list of $B^n_E$. The authors thus propose a new score called the \acrfull{NESH} which looks at the individual nodes in networks $A$ and $B$, and assigns them a score based upon the change of the node's neighborhood in the two networks.

The author's introduce a few components to be included in the \acrshort{NESH} calculation. They define \acrshort{NESH} as:
\begin{equation}\label{eq:NESH}
    \text{\acrshort{NESH}} = 1 - \left(X - \left(Y +Z\right) \right),
\end{equation}
where $X$ is a measure similar to the \acrshort{JEI} but focusing on the node's neighbors, and $Y,Z$ are penalty terms for $X$ that consider exclusive enrichment of the case set $B$ over the control $A$.

If we let $\left[Neighbors\right]^A$ and $\left[Neighbors\right]^B$ be the set of first neighbors for a shared node in $A$ and $B$ respectively, then we define the intersection of the node for set $A$ and $B$ with $\left[Neighbors\right]^A \cap \left[Neighbors\right]^B$; the union as $\left[Neighbors\right]^A \cup \left[Neighbors\right]^B$; the relative complement of set $B$ (objects in $B$ but not in $A$) as $\left[Neighbors\right]^B - \left[Neighbors\right]^A$; and the maximum degree of the nodes in $B$ as $\max\left(k^B\right)$. Accordingly, $X,Y,$ and $Z$ are defined as:
\begin{equation}\label{eq:NESH-vals}
    \begin{split}
        X = \frac{\left[Neighbors\right]^A \cap \left[Neighbors\right]^B}{\left[Neighbors\right]^A \cup \left[Neighbors\right]^B}, \\
        Y = \frac{\left[Neighbors\right]^B - \left[Neighbors\right]^A}{\max \left(k^B\right)}, \\
        Z = \frac{\left[Neighbors\right]^B - \left[Neighbors\right]^A}{\left[Neighbors\right]^B \cup \left[Neighbors\right]^A}.
    \end{split}
\end{equation}
Specifically, $Y$ represents the unique connections in $B$ compared to the possible number of connections that the node can have. $Z$ defines the quantity the exclusive set over the union of the interacting partners.  \acrshort{NESH} is then calculated by substituting Equations \ref{eq:NESH-vals} into Equation \ref{eq:NESH}.
% \begin{equation}
% \begin{split}
%         \text{NESH}_{A\to B} = &  1- \left[  \frac{\left[Neighbors \right]^A \cap \left[Neighbors \right]^B}{\left[Neighbors \right]^A \cup \left[Neighbors \right]^B} - \\
%         & \left( \frac{\left[Neighbors \right]^B - \left[ Neighbors \right]^A }{\max \left(k_i^B\right)} + \frac{\left[Neighbors \right]^B - \left[Neighbors \right]^A}{\left[Neighbors \right]^B \cup \left[Neighbors \right]^A} \right) \right] \\
% \end{split}
% \end{equation}

